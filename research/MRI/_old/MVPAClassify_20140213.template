# generated <creation_time> by MVPAClassify.m

from mvpa2.suite import *
import numpy as np
import scipy.stats as sp
import scipy.io
import nibabel
import random

np.seterr(divide='ignore');
np.seterr(invalid='ignore');

#script parameters
do_zscore             = <do_zscore>
do_custom_zscore      = <do_custom_zscore>
do_spatiotemporal     = <do_spatiotemporal>
do_target_subset      = <do_target_subset>
do_target_blank       = <do_target_blank>
do_average            = <do_average>
do_balancer           = <do_balancer>
do_selection          = <do_selection>
do_selection_fraction = <do_selection_fraction>
do_selection_n        = <do_selection_n>
do_save_selected      = <do_save_selected>
do_permutation_test   = <do_permutation_test>
do_allway             = <do_allway>
do_sensitivities      = <do_sensitivities>
do_twoway             = <do_twoway>
do_mask_balancer      = <do_mask_balancer>
do_mean_control       = <do_mean_control>
nan_dimension         = <nan_dimension>
zscore_attr           = '<zscore_attr>'
zscore_attr_val       = [<zscore_attr_val>]
target_subset         = [<target_subset>]
target_blank          = '<target_blank>'
classifier            = <classifier>(<classifier_param>)
nfold_leaveout        = <nfold_leaveout>
balancer_count        = <balancer_count>
selection_parameter   = <selection_parameter>
permutation_count     = <permutation_count>
path_result           = '<path_result>'
path_attribute        = '<path_attribute>'
path_data             = '<path_data>'
path_mask             = [<path_mask>]
mask_balancer_count   = <mask_balancer_count> if do_mask_balancer else 1

#global variables
out         = {}
out_n       = {}
ds_selected = None
km = kb = n_mask = voxel_min = -1
n_voxel_mask = sub_voxels = []

def MaskSubset(msk,n_voxel):
	'''randomly choose a subset of voxels in a mask
	'''
	msk  = msk.copy()
	nz   = np.nonzero(msk)
	n_nz = len(nz[0])
	
	voxel_keep     = random.sample(range(n_nz),n_voxel)
	voxel_discard  = list(set(range(n_nz)).difference(voxel_keep))
	k_keep         = tuple([nz[k][voxel_keep] for k in range(len(nz))])
	k_discard      = tuple([nz[k][voxel_discard] for k in range(len(nz))])
	msk[k_discard] = False
	
	return msk,voxel_keep
def RecordValue(key,val,idx=[],avg=True,sub_mask=False):
	'''record a value for saving to the output .mat file. if the same value is
	   saved multiple times then an average is calculated.
	'''
	global out,out_n,km,sub_voxels
	
	if idx!=[]:
		if not isinstance(idx,list):
			idx = [idx]
		
		key = key+ '_' + '_'.join(map(str,idx))
	
	if avg:
		if sub_mask:
			if key not in out:
				out_n[key] = np.zeros(n_voxel_mask[km])
				out[key]   = np.zeros([len(val),n_voxel_mask[km]])
				
				out_n[key][sub_voxels] = 1
				out[key][:,sub_voxels] = val
			else:
				out_n[key][sub_voxels] += 1
				out[key][:,sub_voxels] += val
		else:
			if key not in out:
				out_n[key] = 1
				out[key]   = val
			else:
				out_n[key] += 1
				out[key]   += val
	else:
		out[key] = val
def AverageValues():
	for key in out_n:
		if isinstance(out_n[key],int):
			out[key] /= out_n[key]
		else:
			nz = np.nonzero(out_n[key])
			
			out[key][:,nz] /= out_n[key][nz]
def SaveSensitivity(name,sense,ds):
	global km,kb,path_result
	
	idx = [km] if mask_balancer_count==1 else [km,kb]
	key = 'sensitivities_'+name+'_'+'_'.join(map(str,idx))
	
	pre_ext,ext = os.path.splitext(path_result)
	path_out    = pre_ext+'-'+key+'.nii.gz'
	
	ds_sense         = ds[0].copy()
	ds_sense.samples = sense
	
	map2nifti(ds_sense).to_filename(path_out)
def ClassifyOne(dsOrig,msk):
	global out,ds_selected,km,kb,n_mask,n_voxel_mask,voxel_min
	
	#display a status message
	str_mask = str(km+1) + '/' + str(n_mask)
	str_boot = (' (boot ' + str(kb+1) + '/' + str(mask_balancer_count) + ')') if (do_mask_balancer and n_voxel_mask[km]!=voxel_min) else '' 
	print 'classification ' + str_mask + str_boot
	
	#apply the mask
	ds = dsOrig[:,dsOrig.a.mapper.forward1(msk)!=0] if msk!=None else dsOrig.copy()
	
	#remove NaNs
	if nan_dimension==0:
		ds = ds[~np.any(np.isnan(ds),1),:]
	elif nan_dimension==1:
		ds = ds[:,~np.any(np.isnan(ds),0)]
	
	#z-score
	if do_zscore:
		#define our own zscore attribute
		if do_custom_zscore:
			ds.sa[zscore_attr] = array(zscore_attr_val)
		
		zscore(ds, chunks_attr=zscore_attr)
	
	#remove blanks unless we are making an event-related dataset
	if not do_spatiotemporal:
		#keep only the targets we're interested in
		if do_target_subset:
			ds = ds[array([ trg in target_subset for trg in ds.sa.targets ])]
		#exclude the 'blank' target
		if do_target_blank:
			ds = ds[ds.targets!=target_blank]
	
	#average samples within chunks
	if do_average:
		avg = mean_group_sample(['targets','chunks'])
		ds  = ds.get_mapped(avg)
	#spatiotemporal
	if do_spatiotemporal:
		events = find_events(targets=ds.sa.targets, chunks=ds.sa.chunks)
		
		#keep only the targets we're interested in
		if do_target_subset:
			events = [ ev for ev in events if ev['targets'] in target_subset ]
		#exclude the 'blank' target
		if do_target_blank:
			events = [ ev for ev in events if ev['targets']!=target_blank ]
		
		ds     = eventrelated_dataset(ds, events=events)
	
	#classifier
	clf = classifier
	
	#mean control classifier
	if do_mean_control:
		ds.fa['all'] = [1]*len(ds.fa['voxel_indices'])
		mgf          = mean_group_feature(['all'])
		clfm         = MappedClassifier(clf,mgf)
	
	#partitioner
	partitioner = NFoldPartitioner(cvtype=nfold_leaveout)
	
	#balancer
	if do_balancer:
		partitioner = ChainNode([partitioner,Balancer(attr='targets',count=balancer_count,limit='partitions',apply_selection=True)],space='partitions')
	
	#feature selection
	if do_selection:
		#get the actual selector
		if do_selection_fraction:
			actual_selector = FractionTailSelector(selection_parameter, mode='select', tail='upper')
		elif do_selection_n:
			actual_selector = FixedNElementTailSelector(selection_parameter, mode='select', tail='upper')
			
		#save a record of the selected voxels
		if do_save_selected:
			ds_selected            = ds[0].copy()
			
			def CaptureSelector(sens):
				global ds_selected
				
				selected = actual_selector(sens)
				
				shp = ds_selected.shape
				ds_selected.samples.resize(shp[0]+1,shp[1])
				ds_selected.samples[-1,selected] = True
				
				return selected
			def ResetSelected():
				global ds_selected
				
				ds_selected.samples = np.ndarray((0,ds_selected.samples.shape[1]))
			def SaveSelected(name):
				global ds_selected
				
				idx = [km] if mask_balancer_count==1 else [km,kb]
				key = 'selected_'+name+'_'+'_'.join(map(str,idx))
				
				pre_ext,ext = os.path.splitext(path_result)
				path_out    = pre_ext+'-'+key+'.nii.gz'
				
				RecordValue('selected_'+name,path_out,idx=idx,avg=False)
				
				map2nifti(ds_selected).to_filename(path_out)
				ResetSelected()
			
			ResetSelected()
			
			selector = CaptureSelector
		else:
			selector = actual_selector
		
		fsel = SensitivityBasedFeatureSelection(OneWayAnova(enable_ca=['raw_results']),selector,enable_ca=['sensitivity'])
		clf  = FeatureSelectionClassifier(clf, fsel)
		
		if do_mean_control:
			clfm = FeatureSelectionClassifier(clfm, fsel)
	
	#cross validation
	if do_permutation_test:
		permutator = AttributePermutator('targets', count=permutation_count)
		distr_est  = MCNullDist(permutator, tail='right')#, enable_ca=['dist_samples'])
		cvte       = CrossValidation(clf, partitioner, errorfx=lambda p, t: np.mean(p == t), postproc=mean_sample(), null_dist=distr_est, enable_ca=['stats'])
		
		if do_mean_control:
			cvtem = CrossValidation(clfm, partitioner, errorfx=lambda p, t: np.mean(p == t), postproc=mean_sample(), null_dist=distr_est, enable_ca=['stats'])
	else:
		cvte = CrossValidation(clf, partitioner, errorfx=lambda p, t: np.mean(p == t), enable_ca=['stats'])
		
		if do_mean_control:
			cvtem = CrossValidation(clfm, partitioner, errorfx=lambda p, t: np.mean(p == t), enable_ca=['stats'])
	
	RecordValue('target',"\t".join(ds.uniquetargets),idx=km,avg=False)
	
	#allway
	if do_allway:
		#analyze
		res       = cvte(ds)
		confusion = cvte.ca.stats.matrix
		
		#save the selected voxels record
		if do_save_selected:
			SaveSelected('allway')
		
		#sensitivities
		if do_sensitivities:
			if do_balancer:
				sa = RepeatedMeasure(clf.get_sensitivity_analyzer(postproc=maxofabs_sample()),partitioner)
			else:
				sa = SplitClassifier(clf, enable_ca=['stats']).get_sensitivity_analyzer()
			
			sense = l1_normed(sa(ds))
			
			RecordValue('allway_sensitivity',sense,idx=km,sub_mask=True)
			SaveSensitivity('allway',sense,ds)
		
		#save the results
		RecordValue('allway_accuracy',res.samples,idx=km)
		RecordValue('allway_confusion',confusion,idx=km)
		
		if do_permutation_test:
			p_permutation = cvte.ca.null_prob.samples
			RecordValue('allway_pvalue',p_permutation,idx=km)
		
		#mean control
		if do_mean_control:
			res_mean       = cvtem(ds)
			confusion_mean = cvtem.ca.stats.matrix
			
			RecordValue('mean_allway_accuracy',res_mean.samples,idx=km)
			RecordValue('mean_allway_confusion',confusion_mean,idx=km)
			
			if do_permutation_test:
				p_permutation_mean = cvtem.ca.null_prob.samples
				RecordValue('mean_allway_pvalue',p_permutation_mean,idx=km)
	#twoway
	if do_twoway:
		n_target = len(ds.uniquetargets)
		
		for k1 in range(0,n_target):
			for k2 in range(k1+1,n_target):
				target_sub = [ds.uniquetargets[k] for k in [k1,k2]]
				ds_sub     = ds[np.logical_or(*[ds.targets==target for target in target_sub])]
				
				res = cvte(ds_sub)
				
				#save the selected voxels record
				if do_save_selected:
					SaveSelected('twoway_'+str(k1)+'_'+str(k2))
				
				#sensitivities
				if do_sensitivities:
					sa = RepeatedMeasure(clf.get_sensitivity_analyzer(postproc=maxofabs_sample()),partitioner) \
					     if do_balancer else \
					     SplitClassifier(clf, enable_ca=['stats']).get_sensitivity_analyzer()
					
					sense = l1_normed(sa(ds_sub))
					
					RecordValue('twoway_sensitivity',sense,idx=[km,k1,k2],sub_mask=True)
					SaveSensitivity('twoway_'+str(k1)+'_'+str(k2),sense,ds)
				
				#save the results
				RecordValue('twoway_accuracy',res.samples,idx=[km,k1,k2])
				
				if do_permutation_test:
					p_permutation = cvte.ca.null_prob.samples
					RecordValue('twoway_pvalue',p_permutation,idx=[km,k1,k2])
				
				#mean control
				if do_mean_control:
					res_mean       = cvtem(ds_sub)
					
					RecordValue('mean_twoway_accuracy',res_mean.samples,idx=[km,k1,k2])
					
					if do_permutation_test:
						p_permutation_mean = cvtem.ca.null_prob.samples
						RecordValue('mean_twoway_pvalue',p_permutation_mean,idx=[km,k1,k2])

#load the attribute file
attr = SampleAttributes(path_attribute)

#load the data
ds = fmri_dataset(samples=path_data, chunks=attr.chunks, targets=attr.targets)

#load the masks
masks        = path_mask if path_mask==[None] else map(lambda f: nibabel.load(f).get_data(),path_mask)
n_mask       = len(masks)
n_voxel_mask = map(lambda nii: 0 if nii==None else int(sum(sum(sum(nii)))),masks)
voxel_min    = min(n_voxel_mask)

#do each analysis
for km in range(n_mask):
	if do_mask_balancer:
		if n_voxel_mask[km]==voxel_min:
			sub_voxels = range(n_voxel_mask[km])
			ClassifyOne(ds,masks[km])
		else:
			for kb in range(mask_balancer_count):
				#get the mask subset
				msk,sub_voxels = MaskSubset(masks[km],voxel_min)
				
				ClassifyOne(ds,msk)
	else:
		sub_voxels = range(n_voxel_mask[km])
		ClassifyOne(ds,masks[km])

#calculate averages for full_mask outputs
AverageValues()

#save the output
scipy.io.savemat(path_result,out,oned_as='column')
